var documenterSearchIndex = {"docs":
[{"location":"userapi/#User-API","page":"User API","title":"User API","text":"","category":"section"},{"location":"userapi/#Summary","page":"User API","title":"Summary","text":"","category":"section"},{"location":"userapi/#Defining","page":"User API","title":"Defining","text":"","category":"section"},{"location":"userapi/","page":"User API","title":"User API","text":"Defining a problem is done in one of three ways:","category":"page"},{"location":"userapi/","page":"User API","title":"User API","text":"The easiest is to use the Turing.jl or Soss.jl probabilistic programming languages to define a model, which is then automatically useable by MuseInference. See the documentation for TuringMuseProblem and SossMuseProblem. Note that to use either of these, you first need run using Turing or using Soss in addition to using MuseInference in your session. \nIf the problem has theta variables whose domain is (-inftyinfty), and if all necessary gradients can be computed with automatic differentiation, you can use SimpleMuseProblem and specify the posterior and simulation generation code by-hand. \nOtherwise, you can write a custom AbstractMuseProblem. See the Developer API","category":"page"},{"location":"userapi/#Solving","page":"User API","title":"Solving","text":"","category":"section"},{"location":"userapi/","page":"User API","title":"User API","text":"Solving a problem is then done using:","category":"page"},{"location":"userapi/","page":"User API","title":"User API","text":"muse and muse! which compute the MUSE estimate and return a MuseResult object. The muse! version will store the result into an existing MuseResult, and will resume an existing run if the MuseResult already holds some work. This can be useful if e.g. one wishes to run the solver for more steps after an initial solution. Both functions accept a keyword argument get_covariance which if true will also compute the covariance of the MUSE estimate. If false, the covariance can be computed later by manually calling the following two functions:\nget_J! and get_H! which compute the H and J matrices which together give the MUSE covariance, Sigma_rm posterior = H^-1JH^-dagger + Sigma_rm prior. Calling these by hand rather than setting get_covariance=true can be useful as they allow more configurable options. ","category":"page"},{"location":"userapi/#Tuning","page":"User API","title":"Tuning","text":"","category":"section"},{"location":"userapi/","page":"User API","title":"User API","text":"The main tunable parameters to these functions which the user should consider are:","category":"page"},{"location":"userapi/","page":"User API","title":"User API","text":"nsims = 100 — The number of simulations used. MUSE is a stochastic estimate. The error on the central MUSE estimate of theta scales as 1  sqrt N_rm sims cdot sigma(theta). For example, a MUSE solution with nsims=100 will have roughly the same error on the mean as an HMC chain with an effective sample size of 100. The total runtime of MUSE is linear in nsims. A different number of sims can be used for muse, get_J! and get_H! (note that H is generally less realization dependent and can be computed with fewer sims). \n∇z_logLike_atol = 1e-2 — The MUSE estimate involves a maximization of the likelihood over z. This controls the absolute solver error tolerance for z for this maximization. \n(θ_rtol, α) = (0.1, 0.7) — The outermost loop of the MUSE algorithm is solving an equation like f(θ)=0. The θ_rtol parameter sets the error tolerance for this solution for theta, relative to an estimate of its uncertainty. The default 0.1 means the solution is good to 01sigma_theta. The α parameter is a step-size for this solver. If a good starting guess for theta is unavailable, this may need to be reduced for the solver to converge. ","category":"page"},{"location":"userapi/","page":"User API","title":"User API","text":"The defaults given above should give reasonable results, but may need tweaking for individual problems. A good strategy is to reduce ∇z_logLike_atol, θ_rtol, and α to ensure a stable solution, then experiment with which can be increased without screwing up the result. The convergence properties are not largely affected by nsims so this can be lowered during this experimentation to offset the longer run-time.","category":"page"},{"location":"userapi/#Parallelizing","page":"User API","title":"Parallelizing","text":"","category":"section"},{"location":"userapi/","page":"User API","title":"User API","text":"MUSE is trivially parallelizable over the nsims simulations which are averaged over in the algorithm. The functions muse, get_J!, and get_H! can all be passed a keyword argument pmap which will be used for mapping over the simulations. For example, pmap=Distributed.pmap can be used to use Julia's Distributed map across different processes. ","category":"page"},{"location":"userapi/#Index","page":"User API","title":"Index","text":"","category":"section"},{"location":"userapi/","page":"User API","title":"User API","text":"Pages = [\"userapi.md\"]","category":"page"},{"location":"userapi/#Contents","page":"User API","title":"Contents","text":"","category":"section"},{"location":"userapi/","page":"User API","title":"User API","text":"TuringMuseProblem\nSossMuseProblem\nSimpleMuseProblem\nmuse\nmuse!\nget_J!\nget_H!\nMuseResult","category":"page"},{"location":"userapi/#MuseInference.TuringMuseProblem","page":"User API","title":"MuseInference.TuringMuseProblem","text":"TuringMuseProblem(model; params, autodiff = Turing.ADBACKEND)\n\nSpecify a MUSE problem with a Turing model.\n\nThe Turing model should be conditioned on the variables which comprise the \"data\", and all other variables should be unconditioned. By default, any parameter which doesn't depend on another parameter will be estimated by MUSE, but this can be overridden by passing params as a list of symbols. All other non-conditioned and non-params variables will be considered the latent space.\n\nThe autodiff parameter should be either MuseInference.ForwardDiffBackend() or MuseInference.ZygoteBackend(), specifying which library to use for automatic differenation. The default uses whatever the global Turing.ADBACKEND is currently set to.\n\nExample\n\n# toy hierarchical model\nTuring.@model function toy()\n    σ ~ Uniform(0, 1)\n    θ ~ Normal(0, σ)\n    z ~ MvNormal(zeros(512), exp(σ/2)*I)\n    w ~ MvNormal(z, I)\n    x ~ MvNormal(w, I)\n    y ~ MvNormal(x, I)\n    (;σ, θ, z, w, x, y)\nend\nsim = toy()()\nmodel = toy() | (;sim.x, sim.y)\nprob = TuringMuseProblem(model, params=(:σ, :θ))\n\n# get solution\nresult = muse(prob, (σ=0.5, θ=0))\n\nHere we have chosen (σ, θ) to be the parameters which MuseInference will estimate (note that the default would have only chosen σ). The observed data are (x,y) and the remaining (z,w) are the latent space.\n\nnote: Note\nYou can also call muse, etc... directly on the model, e.g. muse(model, (σ=0.5, θ=0)), in which case the parameter names params will be read from the keys of provided the starting point.\n\nnote: Note\nThe model function cannot have any of the random variables as  arguments, although it can have other parameters as arguments. E.g.,# not OK\n@model function toy(x)\n    x ~ Normal()\n    ...\nend\n\n# OK\n@model function toy(σ)\n    x ~ Normal(σ)\n    ...\nend\n\n\n\n","category":"type"},{"location":"userapi/#MuseInference.SossMuseProblem","page":"User API","title":"MuseInference.SossMuseProblem","text":"SossMuseProblem(model; params, autodiff = ForwardDiffBackend())\n\nSpecify a MUSE problem with a Soss model.\n\nThe Soss model should be conditioned on the variables which comprise the \"data\", and all other variables should be unconditioned. By default, any parameter which doesn't depend on another parameter will be estimated by MUSE, but this can be overridden by passing params as a list of symbols. All other non-conditioned and non-params variables will be considered the latent space.\n\nThe autodiff parameter should be either MuseInference.ForwardDiffBackend() or MuseInference.ZygoteBackend(), specifying which library to use for automatic differenation.\n\nExample\n\n# toy hierarchical problem, using MeasureTheory distributions\nfunnel = Soss.@model (σ) begin\n    θ ~ MeasureTheory.Normal(0, σ)\n    z ~ MeasureTheory.Normal(0, exp(θ/2)) ^ 512\n    x ~ For(z) do zᵢ\n        MeasureTheory.Normal(zᵢ, 1)\n    end\nend\n\nsim = rand(funnel(3))\nmodel = funnel(3) | (;sim.x)\nprob = SossMuseProblem(model)\n\n# get solution\nresult = muse(prob, (θ=0,))\n\nnote: Note\nYou can also call muse, etc... directly on the model, e.g. muse(model, (θ=0,)), in which case the parameter names params will be read from the keys of provided the starting point.\n\n\n\n","category":"type"},{"location":"userapi/#MuseInference.SimpleMuseProblem","page":"User API","title":"MuseInference.SimpleMuseProblem","text":"SimpleMuseProblem(x, sample_x_z, logLike, logPriorθ=(θ->0); ad=AD.ForwardDiffBackend())\n\nSpecify a MUSE problem by providing the simulation and posterior evaluation code by-hand. The argument x should be the observed data. The function sample_x_z should have signature:\n\nfunction sample_x_z(rng::AbstractRNG, θ)\n    # ...\n    return (;x, z)\nend\n\nand return a joint sample of data x and latent space z. The function logLike should have signature:\n\nfunction logLike(x, z, θ) \n    # return log likelihood\nend\n\nand return the likelihood logmathcalP(xztheta) for your problem. The optional function logPriorθ should have signature:\n\nfunction logPriorθ(θ)\n    # return log prior\nend\n\nand should return the prior logmathcalP(theta) for your problem. The autodiff parameter should be either MuseInference.ForwardDiffBackend() or MuseInference.ZygoteBackend(), specifying which library to use for automatic differenation through logLike.\n\nAll variables (x, z, θ) can be any types which support basic arithmetic.\n\nExample\n\n# 512-dimensional noisy funnel\nprob = SimpleMuseProblem(\n    rand(512),\n    function sample_x_z(rng, θ)\n        z = rand(rng, MvNormal(zeros(512), exp(θ)*I))\n        x = rand(rng, MvNormal(z, I))\n        (;x, z)\n    end,\n    function logLike(x, z, θ)\n        -(1//2) * (sum((x .- z).^2) + sum(z.^2) / exp(θ) + 512*θ)\n    end, \n    function logPrior(θ)\n        -θ^2/(2*3^2)\n    end;\n    autodiff = MuseInference.ZygoteBackend()\n)\n\n# get solution\nmuse(prob, (θ=1,))\n\n\n\n","category":"type"},{"location":"userapi/#MuseInference.muse","page":"User API","title":"MuseInference.muse","text":"muse(prob::AbstractMuseProblem, θ₀; kwargs...)\nmuse!(result::MuseResult, prob::AbstractMuseProblem, [θ₀=nothing]; kwargs...)\n\nRun the MUSE estimate. The muse! form resumes an existing result. If the muse form is used instead, θ₀ must give a starting guess for theta.\n\nSee MuseResult for description of return value. \n\nKeyword arguments:\n\nrng — Random number generator to use. Taken from result.rng or Random.default_rng() if not passed. \nz₀ — Starting guess for the latent space MAP.\nmaxsteps = 50 — Maximum number of iterations. \nθ_rtol = 1e-2 — Error tolerance on theta relative to its standard deviation.\n∇z_logLike_atol = 1e-2 — Absolute tolerance on the z-gradient at the MAP solution. \nnsims = 100 — Number of simulations. \nα = 0.7 — Step size for root-finder. \nprogress = false — Show progress bar.\npool :: AbstractWorkerPool — Worker pool for parallelization.\nregularize = identity — Apply some regularization after each step. \nH⁻¹_like = nothing — Initial guess for the inverse Jacobian of s^rm MUSE(theta)\nH⁻¹_update — How to update H⁻¹_like. Should be :sims, :broyden, or :diagonal_broyden. \nbroyden_memory = Inf — How many past steps to keep for Broyden updates. \ncheckpoint_filename = nothing — Save result to a file after each iteration. \nget_covariance = false — Also call get_H and get_J to get the full covariance.\n\n\n\n","category":"function"},{"location":"userapi/#MuseInference.muse!","page":"User API","title":"MuseInference.muse!","text":"See muse.\n\n\n\n","category":"function"},{"location":"userapi/#MuseInference.get_J!","page":"User API","title":"MuseInference.get_J!","text":"get_J!(result::MuseResult, prob::AbstractMuseProblem, [θ₀=nothing]; kwargs...)\n\nCompute the J matrix, which is part of the MUSE covariance computation (see Millea & Seljak, 2021). \n\nPositional arguments: \n\nresult — MuseResult into which to store result\nprob — AbstractMuseProblem being solved\nθ₀ — the θ at which to evaluate J (default: result.θ if it exists, otherwise θ₀ must be given)\n\nKeyword arguments:\n\nz₀ — Starting guess for the latent space MAPs. Defaults to random sample from prior.\n∇z_logLike_atol = 1e-2 — Absolute tolerance on the z-gradient at the MAP solution. \nrng — Random number generator to use. Taken from result.rng or Random.default_rng() if not passed. \nnsims — How many simulations to average over (default: 100)\npmap — Parallel map function. \nprogress — Show progress bar (default: false), \nskip_errors — Ignore any MAP that errors (default: false)\ncovariance_method — A CovarianceEstimator used to compute J (default: SimpleCovariance(corrected=true))\n\n\n\n","category":"function"},{"location":"userapi/#MuseInference.get_H!","page":"User API","title":"MuseInference.get_H!","text":"get_H!(result::MuseResult, prob::AbstractMuseProblem, [θ₀=nothing]; kwargs...)\n\nCompute the H matrix, which is part of the MUSE covariance computation (see Millea & Seljak, 2021). \n\nPositional arguments: \n\nresult — MuseResult into which to store result\nprob — AbstractMuseProblem being solved\nθ₀ — the θ at which to evaluate H (default: result.θ if it exists, otherwise θ₀ must be given)\n\nKeyword arguments:\n\nz₀ — Starting guess for the latent space MAPs. Defaults to random sample from prior.\n∇z_logLike_atol = 1e-2 — Absolute tolerance on the z-gradient at the MAP solution. \nrng — Random number generator to use. Taken from result.rng or Random.default_rng() if not passed. \nnsims — How many simulations to average over (default: 10)\npmap — Parallel map function. \nprogress — Show progress bar (default: false), \nskip_errors — Ignore any MAP that errors (default: false)\nfdm — A FiniteDifferenceMethod used to compute the finite difference Jacobian of AD gradients involved in computing H (defaults to: FiniteDifferences.central_fdm(3,1))\nstep — A guess for the finite difference step-size (defaults to 0.1σ for each parameter using J to estimate σ; for this reason its recommended to run get_J! before get_H!). Is only a guess because different choices of fdm may adapt this.\nimplicit_diff — Whether to use experimental implicit differentiation, rather than finite differences. Will require 2nd order AD through your logLike so pay close attention to your prob.autodiff. Either AD.HigherOrderBackend((AD.ForwardDiffBackend(), AD.ZygoteBackend())) or AD.ForwardDiffBackend() are recommended (default: false)\n\n\n\n","category":"function"},{"location":"userapi/#MuseInference.MuseResult","page":"User API","title":"MuseInference.MuseResult","text":"Stores the result of a MUSE run. Can be constructed by-hand as MuseResult() and passed to any of the inplace muse!, get_J!, or get_H!.\n\nFields:\n\nθ — The estimate of the theta parameters. \nΣ, Σ⁻¹ — The approximate covariance of theta and its inverse. \nH, J — The H and J matrices which form the covariance (see Millea & Seljak, 2021)\ngs — The MAP gradient sims used to compute J.\nHs — The jacobian sims used to compute H. \ndist — A Normal or MvNormal built from θ and Σ, for convenience. \nhistory — Internal diagnostic info from the run. \nrng — RNG used to generate sims for this run (so the same sims can be reused if resuming later).\ntime — Total Millisecond wall-time spent computing the result.\n\n\n\n","category":"type"},{"location":"#MuseInference.jl","page":"MuseInference.jl","title":"MuseInference.jl","text":"","category":"section"},{"location":"","page":"MuseInference.jl","title":"MuseInference.jl","text":"(Image: ) (Image: )","category":"page"},{"location":"","page":"MuseInference.jl","title":"MuseInference.jl","text":"(Image: )","category":"page"},{"location":"","page":"MuseInference.jl","title":"MuseInference.jl","text":"The Marginal Unbiased Score Expansion (MUSE) method is a generic tool for hierarchical Bayesian inference. MUSE performs approximate marginalization over arbitrary non-Gaussian and high-dimensional latent spaces, providing Gaussianized constraints on hyper parameters of interest. It is much faster than exact methods like Hamiltonian Monte Carlo (HMC), and requires no user input like many Variational Inference (VI), and Likelihood-Free Inference (LFI) or Simulation-Based Inference (SBI) methods. It excels in high-dimensions, which challenge these other methods. It is approximate, so its results may need to be spot-checked against exact methods, but it is itself exact in asymptotic limit of a large number of data modes contributing to each hyperparameter, or in the limit of Gaussian joint likelihood regardless the number of data modes. For more details, see Millea & Seljak, 2021.","category":"page"},{"location":"","page":"MuseInference.jl","title":"MuseInference.jl","text":"MUSE works on standard hierarchical problems, where the likelihood is of the form:","category":"page"},{"location":"","page":"MuseInference.jl","title":"MuseInference.jl","text":"mathcalP(xtheta) = int rm dz  mathcalP(xztheta)","category":"page"},{"location":"","page":"MuseInference.jl","title":"MuseInference.jl","text":"In our notation, x are the observed variables (the \"data\"), z are unobserved \"latent\" variables, and theta are some \"hyperparameters\" of interest. MUSE is applicable when the goal of the analysis is to estimate the hyperparameters, theta, but otherwise, the latent variables, z, do not need to be inferred (only marginalized out via the integral above). ","category":"page"},{"location":"","page":"MuseInference.jl","title":"MuseInference.jl","text":"The only requirements to run MUSE on a particular problem are that forward simulations from mathcalP(xztheta) can be generated, and gradients of the joint likelihood, mathcalP(xztheta) with respect to z and theta can be computed. The marginal likelihood is never required, so MUSE could be considered a form of LFI/SBI. ","category":"page"},{"location":"#Install","page":"MuseInference.jl","title":"Install","text":"","category":"section"},{"location":"","page":"MuseInference.jl","title":"MuseInference.jl","text":"MuseInference.jl is a Julia package for computing the MUSE estimate. To install it, run the following from the Julia package prompt:","category":"page"},{"location":"","page":"MuseInference.jl","title":"MuseInference.jl","text":"pkg> add https://github.com/marius311/MuseInference.jl","category":"page"},{"location":"#Example","page":"MuseInference.jl","title":"Example","text":"","category":"section"},{"location":"","page":"MuseInference.jl","title":"MuseInference.jl","text":"The easiest way to use MuseInference is with problems defined via the Probabilistic Programming Language, Turing.jl.","category":"page"},{"location":"","page":"MuseInference.jl","title":"MuseInference.jl","text":"First, load up the packages we'll need:","category":"page"},{"location":"","page":"MuseInference.jl","title":"MuseInference.jl","text":"using MuseInference, Turing\nusing AbstractDifferentiation, Dates, LinearAlgebra, Printf, PyPlot, Random, Zygote\nTuring.setadbackend(:zygote)\nPyPlot.ioff() # hide\nusing Logging # hide\nLogging.disable_logging(Logging.Info) # hide\nTuring.AdvancedVI.PROGRESS[] = false # hide\nTuring.PROGRESS[] = false # hide\nnothing # hide","category":"page"},{"location":"","page":"MuseInference.jl","title":"MuseInference.jl","text":"As an example, consider the following hierarchical problem, which has the classic Neal's Funnel problem embedded in it. Neal's funnel is a standard example of a non-Gaussian latent space which HMC struggles to sample efficiently without extra tricks. Specifically, we consider the model defined by:","category":"page"},{"location":"","page":"MuseInference.jl","title":"MuseInference.jl","text":"beginaligned\ntheta sim rm Normal(03)  \nz_i sim rm Normal(0exp(theta2))  \nx_i sim rm Normal(z_i 1)\nendaligned","category":"page"},{"location":"","page":"MuseInference.jl","title":"MuseInference.jl","text":"for i=12048. This problem can be described by the following Turing model:","category":"page"},{"location":"","page":"MuseInference.jl","title":"MuseInference.jl","text":"@model function funnel()\n    θ ~ Normal(0, 3)\n    z ~ MvNormal(zeros(2048), exp(θ)*I)\n    x ~ MvNormal(z, I)\nend\nnothing # hide","category":"page"},{"location":"","page":"MuseInference.jl","title":"MuseInference.jl","text":"Next, let's choose a true value of theta=0 and generate some simulated data which we'll use as \"observations\":","category":"page"},{"location":"","page":"MuseInference.jl","title":"MuseInference.jl","text":"Random.seed!(1)\n(;x) = rand(funnel() | (θ=0,))\nmodel = funnel() | (;x)\nnothing # hide","category":"page"},{"location":"","page":"MuseInference.jl","title":"MuseInference.jl","text":"We can run HMC on the problem to compute the \"true\" answer to compare against:","category":"page"},{"location":"","page":"MuseInference.jl","title":"MuseInference.jl","text":"Random.seed!(2)\nsample(model, NUTS(10,0.65,init_ϵ=0.5), 10); # warmup # hide\nchain = @time sample(model, NUTS(100, 0.65, init_ϵ=0.5), 500)\nnothing # hide","category":"page"},{"location":"","page":"MuseInference.jl","title":"MuseInference.jl","text":"We next compute the MUSE estimate for the same problem. To reach the same Monte Carlo error as HMC, the number of MUSE simulations should be the same as the effective sample size of the chain we just ran. This is:","category":"page"},{"location":"","page":"MuseInference.jl","title":"MuseInference.jl","text":"nsims = round(Int, ess_rhat(chain)[:θ,:ess])","category":"page"},{"location":"","page":"MuseInference.jl","title":"MuseInference.jl","text":"Running the MUSE estimate, ","category":"page"},{"location":"","page":"MuseInference.jl","title":"MuseInference.jl","text":"muse(model, 0; nsims, get_covariance=true) # warmup # hide\nRandom.seed!(3)\nmuse_result = @time muse(model, 0; nsims, get_covariance=true)\nnothing # hide","category":"page"},{"location":"","page":"MuseInference.jl","title":"MuseInference.jl","text":"Lets also try mean-field variational inference (MFVI) to compare to another approximate method.","category":"page"},{"location":"","page":"MuseInference.jl","title":"MuseInference.jl","text":"Random.seed!(4)\nvi(model, ADVI(10, 10)) # warmup # hide\nt_vi = @time @elapsed vi_result = vi(model, ADVI(10, 1000))\nnothing # hide","category":"page"},{"location":"","page":"MuseInference.jl","title":"MuseInference.jl","text":"Now let's plot the different estimates. In this case, MUSE gives a nearly perfect answer at a fraction of the computational cost. MFVI struggles in both speed and accuracy by comparison.","category":"page"},{"location":"","page":"MuseInference.jl","title":"MuseInference.jl","text":"figure(figsize=(6,5)) # hide\naxvline(0, c=\"k\", ls=\"--\", alpha=0.5)\nhist(collect(chain[\"θ\"][:]), density=true, bins=15, label=@sprintf(\"HMC (%.1f seconds)\", chain.info.stop_time - chain.info.start_time))\nθs = range(-0.5,0.5,length=1000)\nplot(θs, pdf.(muse_result.dist, θs), label=@sprintf(\"MUSE (%.1f seconds)\", (muse_result.time / Millisecond(1000))))\nplot(θs, pdf.(Normal(vi_result.dist.m[1], vi_result.dist.σ[1]), θs), label=@sprintf(\"MFVI (%.1f seconds)\", t_vi))\nlegend()\nxlabel(L\"\\theta\")\nylabel(L\"\\mathcal{P}(\\theta\\,|\\,x)\")\ntitle(\"2048-dimensional noisy funnel\")\ngcf() # hide","category":"page"},{"location":"","page":"MuseInference.jl","title":"MuseInference.jl","text":"The timing[1] difference is indicative of the speedups over HMC that are possible. These get even more dramatic as we increase dimensionality, which is why MUSE really excels on high-dimensional problems.","category":"page"},{"location":"","page":"MuseInference.jl","title":"MuseInference.jl","text":"[1]: Julia experts may wonder if the @time calls above aren't just mainly timing compilation, but this document is generated with hidden \"warmup\" calls which ensure that only the runtime is measured.","category":"page"},{"location":"","page":"MuseInference.jl","title":"MuseInference.jl","text":"","category":"page"},{"location":"","page":"MuseInference.jl","title":"MuseInference.jl","text":"It is also possible to use MuseInference without Turing. The MUSE estimate requires three things:","category":"page"},{"location":"","page":"MuseInference.jl","title":"MuseInference.jl","text":"A function which samples from the joint likelihood, mathcalP(xztheta), with signature:\nfunction sample_x_z(rng::AbstractRNG, θ)\n    # ...\n    return (;x, z)\nend\nwhere rng is an AbstractRNG object which should be used when generating random numbers, θ are the parameters, and return value should be a named tuple (;x, z). \nA function which computes the joint likelihood, mathcalP(xztheta), with signature:\nfunction logLike(x, z, θ) \n    # return log likelihood\nend\nA user-specifiable automatic differentiation library will be used to take gradients of this function. \nA function which computes the prior, mathcalP(theta), with signature:\nfunction logPrior(θ)\n    # return log prior\nend\nIf none is provided, the prior is assumed uniform. ","category":"page"},{"location":"","page":"MuseInference.jl","title":"MuseInference.jl","text":"In all cases, x, z, and θ, can be of any type which supports basic arithmetic, including scalars, Vectors, special vector types like ComponentArrays, etc...","category":"page"},{"location":"","page":"MuseInference.jl","title":"MuseInference.jl","text":"We can compute the MUSE estimate for the same funnel problem as above. To do so, first we create a SimpleMuseProblem object which specifies the three functions:","category":"page"},{"location":"","page":"MuseInference.jl","title":"MuseInference.jl","text":"prob = SimpleMuseProblem(\n    x,\n    function sample_x_z(rng, θ)\n        z = rand(rng, MvNormal(zeros(2048), exp(θ)*I))\n        x = rand(rng, MvNormal(z, I))\n        (;x, z)\n    end,\n    function logLike(x, z, θ)\n        -(1//2) * (sum((x .- z).^2) + sum(z.^2) / exp(θ) + 2048*θ)\n    end, \n    function logPrior(θ)\n        -θ^2/(2*3^2)\n    end;\n    autodiff = AD.ZygoteBackend()\n)\nnothing # hide","category":"page"},{"location":"","page":"MuseInference.jl","title":"MuseInference.jl","text":"And compute the estimate:","category":"page"},{"location":"","page":"MuseInference.jl","title":"MuseInference.jl","text":"Random.seed!(3)\nmuse_result_manual = muse(prob, 0; nsims, get_covariance=true)\nnothing # hide","category":"page"},{"location":"","page":"MuseInference.jl","title":"MuseInference.jl","text":"This gives the same answer as before:","category":"page"},{"location":"","page":"MuseInference.jl","title":"MuseInference.jl","text":"(muse_result.θ[1], muse_result_manual.θ)","category":"page"},{"location":"","page":"MuseInference.jl","title":"MuseInference.jl","text":"","category":"page"},{"location":"devapi/#Developer-API","page":"Developer API","title":"Developer API","text":"","category":"section"},{"location":"devapi/#Summary","page":"Developer API","title":"Summary","text":"","category":"section"},{"location":"devapi/","page":"Developer API","title":"Developer API","text":"This page describes how to create a custom AbstractMuseProblem type. You might want to do this if you are creating a new interface between MuseInference and some PPL package that is not currently supported, or if you have a problem which SimpleMuseProblem cannot handle. You do not need to do this if you have a model you can describe with a supported PPL package (Turing or Soss), or if SimpleMuseProblem is sufficient for you. ","category":"page"},{"location":"devapi/","page":"Developer API","title":"Developer API","text":"As a reminder, MUSE works on joint posteriors of the form,","category":"page"},{"location":"devapi/","page":"Developer API","title":"Developer API","text":"mathcalP(xztheta) mathcalP(theta)","category":"page"},{"location":"devapi/","page":"Developer API","title":"Developer API","text":"where x represents one or more observed variables, z represents one or more latent variables, and theta represents one of more hyper parameters which will be estimated by MUSE. The interface below more or less involves mapping these variables from your original problem to the form expected by MuseInference. The minimum functions you need to implement to get MUSE working are:","category":"page"},{"location":"devapi/","page":"Developer API","title":"Developer API","text":"MuseInference.sample_x_z to sample from (xz) sim mathcalP(xztheta)\nMuseInference.∇θ_logLike to compute the gradient,nabla_theta logmathcalP(xztheta).\nMuseInference.logLike_and_∇z_logLike to compute (logmathcalP(xztheta) nabla_z logmathcalP(xztheta)).\nMuseInference.logPriorθ (optional) to compute the prior, logmathcalP(theta) (defaults to zero).","category":"page"},{"location":"devapi/","page":"Developer API","title":"Developer API","text":"The (xztheta) can be any types which support basic arithmetic. ","category":"page"},{"location":"devapi/","page":"Developer API","title":"Developer API","text":"Internally in the function MuseInference.ẑ_at_θ, MuseInference does a maximization over z using logLike_and_∇z_logLike and Optim.jl's LBFGS solver. If you'd like, you can customize the entire maximization by directly implementing ẑ_at_θ yourself, in which case you do not need to implement logLike_and_∇z_logLike at all. ","category":"page"},{"location":"devapi/","page":"Developer API","title":"Developer API","text":"MuseInference assumes z and theta have support on (-inftyinfty). For some problems, this may not be the case, e.g. if you have a theta sim rm LogNormal, then theta only has support on (0infty). If this is the case for your problem, you have three options:","category":"page"},{"location":"devapi/","page":"Developer API","title":"Developer API","text":"If none of the internal solvers \"bump up\" against the edges of the support, then you don't need to do anything else.\nOutside of MuseInference, you can perform a change-of-variables for theta and/or z such that the transformed variables have support on (-inftyinfty), and implement the functions above in terms of the transformed variables. In this case, MuseInference never knows (or needs to know) about the transformation, and the returned estimate of theta will be an estimate of the transformed theta (which if desired you can transform back outside of MuseInference).\nIf you would like MuseInference itself to return an estimate of the untransformed theta, then you can implement:\nMuseInference.transform_θ\nMuseInference.inv_transform_θ\nThe extended forms of MuseInference.∇θ_logLike and MuseInference.logPriorθ which take a θ_space argument.","category":"page"},{"location":"devapi/","page":"Developer API","title":"Developer API","text":"MuseInference doesn't provide an estimate of z, so if necessary, you should handle transforming it to (-inftyinfty) outside of MuseInference.","category":"page"},{"location":"devapi/","page":"Developer API","title":"Developer API","text":"Once your define the custom AbstractMuseProblem, you can use MuseInference.check_self_consistency to run some self-consistency checks on it.","category":"page"},{"location":"devapi/#Index","page":"Developer API","title":"Index","text":"","category":"section"},{"location":"devapi/","page":"Developer API","title":"Developer API","text":"Pages = [\"devapi.md\"]","category":"page"},{"location":"devapi/#Contents","page":"Developer API","title":"Contents","text":"","category":"section"},{"location":"devapi/","page":"Developer API","title":"Developer API","text":"MuseInference.transform_θ\nMuseInference.inv_transform_θ\nMuseInference.sample_x_z\nMuseInference.∇θ_logLike\nMuseInference.logLike_and_∇z_logLike\nMuseInference.logPriorθ\nMuseInference.ẑ_at_θ\nMuseInference.standardizeθ\nMuseInference.check_self_consistency","category":"page"},{"location":"devapi/#MuseInference.transform_θ","page":"Developer API","title":"MuseInference.transform_θ","text":"transform_θ(prob::AbstractMuseProblem, θ)\n\nMap θ to a space where its domain is (-inftyinfty). Defaults to identity function. \n\n\n\n","category":"function"},{"location":"devapi/#MuseInference.inv_transform_θ","page":"Developer API","title":"MuseInference.inv_transform_θ","text":"inv_transform_θ(prob::AbstractMuseProblem, θ)\n\nMap θ from the space where its domain is (-inftyinfty) back to the original space. Defaults to identity function.\n\n\n\n","category":"function"},{"location":"devapi/#MuseInference.sample_x_z","page":"Developer API","title":"MuseInference.sample_x_z","text":"Return a tuple (x,z) with data x and latent space z which are a sample from the joint likelihood, given θ. The signature of the function should be:\n\nsample_x_z(prob::AbstractMuseProblem, rng::AbstractRNG, θ)\n\nRandom numbers generated internally should use rng.\n\nThe θ argument to this function will always be in the un-transfored θ space.\n\n\n\n","category":"function"},{"location":"devapi/#MuseInference.∇θ_logLike","page":"Developer API","title":"MuseInference.∇θ_logLike","text":"Return the gradient of the joint log likelihood with respect to hyper parameters θ, evaluated at data x and latent space z. The signature of the function should be:\n\n∇θ_logLike(prob::AbstractMuseProblem, x, z, θ)\n\nIf the problem needs a transformation of θ to map its domain to (-inftyinfty), then it should instead implement:\n\n∇θ_logLike(prob::AbstractMuseProblem, x, z, θ, θ_space)\n\nwhere θ_space will be either Transformedθ() or UnTransformedθ(). In this case, the θ argument will be passed in the space given by θ_space and the gradient should be w.r.t. to θ in that space.\n\nz must have domain (-inftyinfty). If a transformation is required to make this the case, that should be handled internal to this function and z will always refer to the transformed z.\n\n\n\n","category":"function"},{"location":"devapi/#MuseInference.logLike_and_∇z_logLike","page":"Developer API","title":"MuseInference.logLike_and_∇z_logLike","text":"Return a tuple (logLike, ∇z_logLike) which give the log likelihood and its gradient with respect to the latent space z, evaluated at hyper parameters θ and data x . The signature of the function should be:\n\nlogLike_and_∇z_logLike(prob::AbstractMuseProblem, x, z, θ)\n\nz must have domain (-inftyinfty). If a transformation is required to make this the case, that should be handled internal to this function and z will always refer to the transformed z.\n\nThe θ argument to this function will always be in the un-transfored θ space.\n\nnote: Note\nAlternatively, custom problems can implement ẑ_at_θ directly and forego this method. The default ẑ_at_θ runs LBFGS with Optim.jl using logLike_and_∇z_logLike.\n\n\n\n","category":"function"},{"location":"devapi/#MuseInference.logPriorθ","page":"Developer API","title":"MuseInference.logPriorθ","text":"Return the log-prior at θ. The signature of the function should be:\n\nlogPriorθ(prob::AbstractMuseProblem, θ)\n\nIf the problem needs a transformation of θ to map its domain to (-inftyinfty), then it should instead implement:\n\nlogPriorθ(prob::AbstractMuseProblem, θ, θ_space)\n\nwhere θ_space will be either Transformedθ() or UnTransformedθ(). In this case, the θ argument will be passed in the space given by θ_space. \n\nDefaults to zero log-prior.\n\n\n\n","category":"function"},{"location":"devapi/#MuseInference.ẑ_at_θ","page":"Developer API","title":"MuseInference.ẑ_at_θ","text":"Return the best-fit latent space z given data x and parameters θ. The signature of the function should be: \n\nẑ_at_θ(prob::AbstractMuseProblem, x, z₀, θ; ∇z_logLike_atol)\n\nThe return value should be (ẑ, info) where info can be any extra diagonstic info which will be saved in the MUSE result. \n\nThe θ argument to this function will always be in the un-transfored θ space.\n\nThe z₀ should be used as a starting guess for the solution. \n\nz must have domain (-inftyinfty). If a transformation is required to make this the case, that should be handled internal to this function, and the return value should refer to the transformed z. \n\nThe default implementation of this method uses logLike_and_∇z_logLike and Optim.jl's LBFGS to iteratively maximize the log likelihood. Custom problems are free to override this default if desired, in which case logLike_and_∇z_logLike does not need to be implemented.\n\n\n\n","category":"function"},{"location":"devapi/#MuseInference.standardizeθ","page":"Developer API","title":"MuseInference.standardizeθ","text":"Pre-process a user-provided θ into the data-structure used internally in the computation. E.g. this allow the user to pass a NamedTuple to functions like muse or get_J! while internally converting it to a ComponentVector. The signature of the function should be:\n\nstandardizeθ(prob::AbstractMuseProblem, θ)\n\n\n\n","category":"function"},{"location":"devapi/#MuseInference.check_self_consistency","page":"Developer API","title":"MuseInference.check_self_consistency","text":"check_self_consistency(\n    prob, \n    θ;\n    fdm = central_fdm(3, 1),\n    atol = 1e-3,\n    rng = Random.default_rng(),\n    has_volume_factor = true\n)\n\nChecks the self-consistency of a defined problem at a given θ, e.g. check that inv_transform_θ(prob, transform_θ(prob, θ)) ≈ θ, etc... This is mostly useful as a diagonostic when implementing a new AbstractMuseProblem. \n\nA random x and z are sampled from rng. Finite differences are computed using fdm and atol set the tolerance for ≈. has_volume_factor determines if the transformation includes the logdet jacobian in the likelihood.\n\n\n\n","category":"function"}]
}
